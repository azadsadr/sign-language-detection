{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchsummary\n",
    "import preprocessing\n",
    "import cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in_channels = 1    # 28x28 = 784, size of MNIST images (grayscale)\n",
    "#hidden_size = 100\n",
    "num_classes = 24\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load & preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transforms = torchvision.transforms.Compose([\n",
    "    preprocessing.ToTensor(), \n",
    "])\n",
    "\n",
    "dataset = preprocessing.SignDataset(csv_path=\"data/sign_mnist_train.csv\", transform=my_transforms)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_test = preprocessing.SignDataset(csv_path=\"data/sign_mnist_test.csv\", transform=my_transforms)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(dataloader): <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "----------------------------------\n",
      "data_iter = iter(dataloader)\n",
      "----------------------------------\n",
      "single_batch = data_iter.next()\n",
      "----------------------------------\n",
      "images, labels = single_batch\n"
     ]
    }
   ],
   "source": [
    "print('type(dataloader):', type(dataloader))\n",
    "print('----------------------------------')\n",
    "print('data_iter = iter(dataloader)')\n",
    "data_iter = iter(dataloader)\n",
    "print('----------------------------------')\n",
    "print('single_batch = data_iter.next()')\n",
    "#single_batch = data_iter.next()\n",
    "#print('type(single_batch) :', type(single_batch))\n",
    "#print('len(single_batch)  :', len(single_batch))\n",
    "print('----------------------------------')\n",
    "print('images, labels = single_batch')\n",
    "#images, labels = single_batch\n",
    "#print('images.shape:', images.shape)\n",
    "#print('labels.shape:', labels.shape)\n",
    "#print(type(labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 26, 26]             100\n",
      "         MaxPool2d-2           [-1, 10, 13, 13]               0\n",
      "            Conv2d-3           [-1, 20, 11, 11]           1,820\n",
      "         MaxPool2d-4             [-1, 20, 5, 5]               0\n",
      "            Conv2d-5             [-1, 30, 3, 3]           5,430\n",
      "         Dropout2d-6             [-1, 30, 3, 3]               0\n",
      "            Linear-7                  [-1, 270]          73,170\n",
      "            Linear-8                   [-1, 24]           6,504\n",
      "        LogSoftmax-9                   [-1, 24]               0\n",
      "================================================================\n",
      "Total params: 87,024\n",
      "Trainable params: 87,024\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.09\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 0.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create the network and look at it's text representation\n",
    "net = cnn.ConvNN().to(device)\n",
    "torchsummary.summary(net, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 1.0\n",
      "<class 'numpy.float32'>\n",
      "b: 1.0\n",
      "<class 'numpy.ndarray'>\n",
      "c: tensor(1.)\n",
      "<class 'torch.Tensor'>\n",
      "d: tensor(1)\n",
      "type(d): <class 'torch.Tensor'>\n",
      "d.shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.float32(1)\n",
    "print('a:', a)\n",
    "print(type(a))\n",
    "b = np.array(a)\n",
    "print('b:', b)\n",
    "print(type(b))\n",
    "c = torch.from_numpy(b)\n",
    "print('c:', c)\n",
    "print(type(c))\n",
    "\n",
    "d = torch.from_numpy(b).type(torch.LongTensor)\n",
    "print('d:', d)\n",
    "print('type(d):', type(d))\n",
    "print('d.shape:', d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/2], batch [200/429], loss = 1.8517\n",
      "epoch [1/2], batch [400/429], loss = 1.7552\n",
      "==============================================================\n",
      "epoch [2/2], batch [200/429], loss = 0.9650\n",
      "epoch [2/2], batch [400/429], loss = 0.8346\n",
      "==============================================================\n"
     ]
    }
   ],
   "source": [
    "trained_model = cnn.train(\n",
    "    loader=dataloader, \n",
    "    model=net, \n",
    "    num_epochs=2, \n",
    "    learning_rate=learning_rate, \n",
    "    device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 22554 / 27455 with accuracy 82.15 %\n",
      "Got 4828 / 7172 with accuracy 67.32 %\n"
     ]
    }
   ],
   "source": [
    "cnn.check_accuracy(loader=dataloader, model=trained_model,device=device)\n",
    "cnn.check_accuracy(loader=dataloader_test, model=trained_model, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d6f7e2b19f5583e90f0e1c45935e0e2e666c556fd2ef0f9241dac243ca3abe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
