{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn                   # all neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.nn.functional as F         # parametersless functions, like (some) activation functions\n",
    "import torch.optim as optim             # all optimization algorithms, SGD, Adam, etc\n",
    "from torch.utils.data import DataLoader # gives easier dataset management and creates mini batches\n",
    "import torchvision\n",
    "from torchvision import datasets        # has standard datasets we can import in a nice and easy way\n",
    "from torchvision import transforms      # transformations we can perform on our dataset (data processing)\n",
    "import nbimporter\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import preproc\n",
    "\n",
    "print(\"Pytorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preproc.get_data(\"data/sign_mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x))\n",
    "single_batch = iter(x)\n",
    "Xs, ys = single_batch.next()\n",
    "print(Xs.shape)\n",
    "print(ys.shape)\n",
    "print(type(ys[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in_channels = 1    # 28x28 = 784, size of MNIST images (grayscale)\n",
    "#hidden_size = 100\n",
    "num_classes = 24\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3,3))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3,3))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=30, kernel_size=(3,3))\n",
    "        self.dropout1 = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(30*3*3, 270)\n",
    "        self.fc2 = nn.Linear(270, num_classes)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = F.relu(output)\n",
    "        output = self.pool1(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.pool2(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout1(output)\n",
    "\n",
    "        output = output.view(-1, 30*3*3)\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        return self.softmax(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 26, 26]             100\n",
      "         MaxPool2d-2           [-1, 10, 13, 13]               0\n",
      "            Conv2d-3           [-1, 20, 11, 11]           1,820\n",
      "         MaxPool2d-4             [-1, 20, 5, 5]               0\n",
      "            Conv2d-5             [-1, 30, 3, 3]           5,430\n",
      "         Dropout2d-6             [-1, 30, 3, 3]               0\n",
      "            Linear-7                  [-1, 270]          73,170\n",
      "            Linear-8                   [-1, 24]           6,504\n",
      "        LogSoftmax-9                   [-1, 24]               0\n",
      "================================================================\n",
      "Total params: 87,024\n",
      "Trainable params: 87,024\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.09\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 0.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create the network and look at it's text representation\n",
    "net = ConvNN().to(device)\n",
    "summary(net, (1, 28, 28))\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "\n",
    "# optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[-3.1986, -3.1986, -3.1494,  ..., -3.1670, -3.1986, -3.1986],\n",
      "        [-3.1929, -3.1929, -3.1602,  ..., -3.1768, -3.1929, -3.1929],\n",
      "        [-3.1945, -3.1960, -3.1805,  ..., -3.1705, -3.1960, -3.1960],\n",
      "        ...,\n",
      "        [-3.1968, -3.1961, -3.1598,  ..., -3.1860, -3.1968, -3.1968],\n",
      "        [-3.1936, -3.1936, -3.1850,  ..., -3.1669, -3.1936, -3.1936],\n",
      "        [-3.1957, -3.1957, -3.1781,  ..., -3.1502, -3.1957, -3.1957]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for index, (images, labels) in enumerate(x):\n",
    "    print(images.shape)\n",
    "    outputs = net(images)\n",
    "    print(type(outputs))\n",
    "    print(outputs)\n",
    "    #one_batch = iter(x)\n",
    "    #print(net(images))\n",
    "    #print(torch.nn.functional.one_hot(torch.arange(0, 5), num_classes=5))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4. 18. 17. 17. 10.  1. 16. 18.  4. 15. 14. 15.  6. 16.  1. 11. 18. 23.\n",
      " 15. 21.  7. 19. 21.  8.  4. 13.  7.  7. 10. 13. 20.  5.  0.  4. 11.  3.\n",
      " 23. 14. 13.  0. 16. 21.  5. 16. 10. 10. 11. 18.  0. 23. 15.  3. 13.  7.\n",
      "  8. 12. 22. 19. 10. 20.  5. 19. 23.  3.]\n",
      "torch.Size([64, 24])\n",
      "tensor([[-3.1977, -3.1977, -3.1532,  ..., -3.1797, -3.1977, -3.1977],\n",
      "        [-3.1993, -3.1993, -3.1596,  ..., -3.1659, -3.1993, -3.1993],\n",
      "        [-3.1918, -3.1972, -3.1682,  ..., -3.1291, -3.1972, -3.1972],\n",
      "        ...,\n",
      "        [-3.1956, -3.1956, -3.1531,  ..., -3.1737, -3.1956, -3.1956],\n",
      "        [-3.1947, -3.1947, -3.1717,  ..., -3.1731, -3.1947, -3.1947],\n",
      "        [-3.1957, -3.1957, -3.1677,  ..., -3.1572, -3.1957, -3.1957]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (images, labels) in enumerate(x):\n",
    "    print(np.array(labels))\n",
    "    labels = torch.nn.functional.one_hot(labels.to(torch.int64), num_classes=24)\n",
    "    print(labels.shape)\n",
    "    outputs = net(images)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/azad/Documents/SLD/convNN.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000021?line=24'>25</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(images) \u001b[39m# (batch_size x num_classes)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000021?line=26'>27</a>\u001b[0m \u001b[39m#output = net(X.view(-1, 28*28)) # pass in reshaped batch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000021?line=28'>29</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000021?line=29'>30</a>\u001b[0m \u001b[39m#loss = F.nll_loss(output, y)    # calc and grab loss value\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000021?line=30'>31</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000021?line=31'>32</a>\u001b[0m \u001b[39m# zero previous gradients. you will do this likely every step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000021?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1163\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1164\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1165\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2995\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2996\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(x)\n",
    "        \n",
    "\n",
    "for epoch in range(num_epochs): # no. of full passes (loop) over the data\n",
    "    #running_loss = 0\n",
    "    #print(f'epoch: {epoch+1}')\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(x):\n",
    "        # get data as a list of [images, labels]\n",
    "        # train_loader is a batch of featuresets and labels\n",
    "        # batch_idx : index of the batch\n",
    "        # images    : one batch of features\n",
    "        # labels    : one batch of targets\n",
    "        \n",
    "        # get data to cuda if possible\n",
    "        images = images.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        \n",
    "        # images are in correct shape\n",
    "        # no need to flatten MNIST images like normal neural network\n",
    "        # we did it inside CNN class\n",
    "        #images = print(\"images.shape:\", images.shape)\n",
    "        \n",
    "        # forward propagation\n",
    "        outputs = net(images) # (batch_size x num_classes)\n",
    "        \n",
    "        #output = net(X.view(-1, 28*28)) # pass in reshaped batch\n",
    "\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        #loss = F.nll_loss(output, y)    # calc and grab loss value\n",
    "\n",
    "        # zero previous gradients. you will do this likely every step\n",
    "        optimizer.zero_grad()\n",
    "        # back-propagation\n",
    "        loss.backward()\n",
    "        # gradient descent or adam step (optimize weights)\n",
    "        optimizer.step()\n",
    "\n",
    "        #running_loss += loss.item()\n",
    "\n",
    "\n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(f'epoch [{epoch+1}/{num_epochs}], step [{batch_idx+1}/{n_total_steps}], loss = {loss.item():.4f}')\n",
    "\n",
    "    print(\"loss =\", loss.item()) # print loss. we hope loss (a measure of wrong-ness) declines!\n",
    "    print(\"==============================================================\")\n",
    "\n",
    "\n",
    "#print(f'Training loss: {running_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:\n",
    "* a single `int` – in which case the same value is used for the height and width dimension\n",
    "* a `tuple` of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(\n",
    "    in_channels, \n",
    "    out_channels, \n",
    "    kernel_size, \n",
    "    stride=1, \n",
    "    padding=0, \n",
    "    dilation=1, \n",
    "    groups=1, \n",
    "    bias=True, \n",
    "    padding_mode='zeros', \n",
    "    device=None, \n",
    "    dtype=None\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d6f7e2b19f5583e90f0e1c45935e0e2e666c556fd2ef0f9241dac243ca3abe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
