{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn                   # all neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.nn.functional as F         # parametersless functions, like (some) activation functions\n",
    "import torch.optim as optim             # all optimization algorithms, SGD, Adam, etc\n",
    "from torch.utils.data import DataLoader # gives easier dataset management and creates mini batches\n",
    "import torchvision\n",
    "from torchvision import datasets        # has standard datasets we can import in a nice and easy way\n",
    "from torchvision import transforms      # transformations we can perform on our dataset (data processing)\n",
    "import nbimporter\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import preprocessing2\n",
    "\n",
    "print(\"Pytorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f3d771a11e0>\n"
     ]
    }
   ],
   "source": [
    "x = preprocessing2.get_data(\"data/sign_mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(type(x))\n",
    "single_batch = iter(x)\n",
    "Xs, ys = single_batch.next()\n",
    "print(Xs.shape)\n",
    "print(ys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in_channels = 1    # 28x28 = 784, size of MNIST images (grayscale)\n",
    "#hidden_size = 100\n",
    "num_classes = 24\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3,3))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3,3))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=30, kernel_size=(3,3))\n",
    "        self.dropout1 = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(30*3*3, 270)\n",
    "        self.fc2 = nn.Linear(270, num_classes)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = F.relu(output)\n",
    "        output = self.pool1(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.pool2(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout1(output)\n",
    "\n",
    "        output = output.view(-1, 30*3*3)\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        return self.softmax(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNN2, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10, 20, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(20, 30, 3) \n",
    "        self.dropout1 = nn.Dropout2d()\n",
    "        \n",
    "        self.fc3 = nn.Linear(30 * 3 * 3, 270) \n",
    "        self.fc4 = nn.Linear(270, 26)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "                \n",
    "        x = x.view(-1, 30 * 3 * 3) \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class ConvNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNN1, self).__init__()\n",
    "        \n",
    "        input_shape = 1,28,28\n",
    "        Conv layer 1    :--> UNITS = 128 | KERNEL SIZE = 5 * 5 | STRIDE LENGTH = 1 | ACTIVATION = ReLu\n",
    "        MaxPool layer 1 :--> MAX POOL WINDOW = 3 * 3 | STRIDE = 2\n",
    "        model.add(Conv2D(128,kernel_size=(5,5), strides=1,padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "        model.add(MaxPool2D(pool_size=(3,3),strides=2,padding='same'))\n",
    "\n",
    "        Conv layer 2    :--> UNITS = 64  | KERNEL SIZE = 3 * 3 | STRIDE LENGTH = 1 | ACTIVATION = ReLu\n",
    "        MaxPool layer 2 :--> MAX POOL WINDOW = 2 * 2 | STRIDE = 2\n",
    "        model.add(Conv2D(64,kernel_size=(2,2), strides=1,activation='relu',padding='same'))\n",
    "        model.add(MaxPool2D((2,2),2,padding='same'))\n",
    "\n",
    "        Conv layer 3    :--> UNITS = 32  | KERNEL SIZE = 2 * 2 | STRIDE LENGTH = 1 | ACTIVATION = ReLu\n",
    "        MaxPool layer 3 :--> MAX POOL WINDOW = 2 * 2 | STRIDE = 2\n",
    "        model.add(Conv2D(32,kernel_size=(2,2), strides=1,activation='relu',padding='same'))\n",
    "        model.add(MaxPool2D((2,2),2,padding='same'))\n",
    "                \n",
    "        model.add(Flatten())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(20, 30, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=270, out_features=270, bias=True)\n",
      "  (fc2): Linear(in_features=270, out_features=24, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the network and look at it's text representation\n",
    "model = CNN().to(device)\n",
    "#summary(model, (1, 28, 28))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "\n",
    "# optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(0, 5))\n",
    "print(torch.nn.functional.one_hot(torch.arange(0, 5), num_classes=5))\n",
    "\n",
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  2.  2. 11.  2. 11. 19. 13. 19. 12.  0.  6. 18. 11. 22.  5. 21.  0.\n",
      " 20.  6. 17. 19. 14.  8.  0. 23.  3. 24. 15. 17. 12.  6. 22.  3. 17. 13.\n",
      "  5. 23.  2. 24. 16. 16. 12. 22. 15.  7. 10. 18.  3.  0. 15. 21.  3.  1.\n",
      "  6.  7. 18. 19. 15. 10. 12.  2.  0.  1.]\n",
      "torch.Size([64, 25])\n",
      "tensor([[-3.1756, -3.1643, -3.1601,  ..., -3.1113, -3.1904, -3.1904],\n",
      "        [-3.1770, -3.1586, -3.1529,  ..., -3.1115, -3.1947, -3.1947],\n",
      "        [-3.1950, -3.1794, -3.1351,  ..., -3.0959, -3.1993, -3.1993],\n",
      "        ...,\n",
      "        [-3.1871, -3.1677, -3.1621,  ..., -3.0936, -3.1969, -3.1969],\n",
      "        [-3.1708, -3.1686, -3.1655,  ..., -3.0888, -3.1958, -3.1958],\n",
      "        [-3.1972, -3.1914, -3.1791,  ..., -3.0748, -3.1972, -3.1972]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (images, labels) in enumerate(x):\n",
    "    print(np.array(labels))\n",
    "    labels = torch.nn.functional.one_hot(labels.to(torch.int64), num_classes=24)\n",
    "    print(labels.shape)\n",
    "    outputs = model(images)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/azad/Documents/SLD/convNN.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000022?line=24'>25</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images) \u001b[39m# (batch_size x num_classes)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000022?line=26'>27</a>\u001b[0m \u001b[39m#output = net(X.view(-1, 28*28)) # pass in reshaped batch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000022?line=28'>29</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000022?line=29'>30</a>\u001b[0m \u001b[39m#loss = F.nll_loss(output, y)    # calc and grab loss value\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000022?line=30'>31</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000022?line=31'>32</a>\u001b[0m \u001b[39m# zero previous gradients. you will do this likely every step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/azad/Documents/SLD/convNN.ipynb#ch0000022?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1163\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1164\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1165\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2995\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2996\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(x)\n",
    "        \n",
    "\n",
    "for epoch in range(num_epochs): # no. of full passes (loop) over the data\n",
    "    #running_loss = 0\n",
    "    #print(f'epoch: {epoch+1}')\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(x):\n",
    "        # get data as a list of [images, labels]\n",
    "        # train_loader is a batch of featuresets and labels\n",
    "        # batch_idx : index of the batch\n",
    "        # images    : one batch of features\n",
    "        # labels    : one batch of targets\n",
    "        \n",
    "        # get data to cuda if possible\n",
    "        images = images.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        \n",
    "        # images are in correct shape\n",
    "        # no need to flatten MNIST images like normal neural network\n",
    "        # we did it inside CNN class\n",
    "        #images = print(\"images.shape:\", images.shape)\n",
    "        \n",
    "        # forward propagation\n",
    "        outputs = model(images) # (batch_size x num_classes)\n",
    "        \n",
    "        #output = net(X.view(-1, 28*28)) # pass in reshaped batch\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        #loss = F.nll_loss(output, y)    # calc and grab loss value\n",
    "\n",
    "        # zero previous gradients. you will do this likely every step\n",
    "        optimizer.zero_grad()\n",
    "        # back-propagation\n",
    "        loss.backward()\n",
    "        # gradient descent or adam step (optimize weights)\n",
    "        optimizer.step()\n",
    "\n",
    "        #running_loss += loss.item()\n",
    "\n",
    "\n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(f'epoch [{epoch+1}/{num_epochs}], step [{batch_idx+1}/{n_total_steps}], loss = {loss.item():.4f}')\n",
    "\n",
    "    print(\"loss =\", loss.item()) # print loss. we hope loss (a measure of wrong-ness) declines!\n",
    "    print(\"==============================================================\")\n",
    "\n",
    "\n",
    "#print(f'Training loss: {running_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:\n",
    "* a single `int` – in which case the same value is used for the height and width dimension\n",
    "* a `tuple` of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(\n",
    "    in_channels, \n",
    "    out_channels, \n",
    "    kernel_size, \n",
    "    stride=1, \n",
    "    padding=0, \n",
    "    dilation=1, \n",
    "    groups=1, \n",
    "    bias=True, \n",
    "    padding_mode='zeros', \n",
    "    device=None, \n",
    "    dtype=None\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d6f7e2b19f5583e90f0e1c45935e0e2e666c556fd2ef0f9241dac243ca3abe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
